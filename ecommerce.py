# -*- coding: utf-8 -*-
"""Perqara Test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jveFuBPHXX46lNdTEeIkJWtobM8uSR5y
"""

#install the required libraries

!pip install category_encoders

"""# Data Preparation

## Data Loading
"""

# Import library

import numpy as np
import pandas as pd

import warnings
warnings.filterwarnings('ignore')

df_customer = pd.read_csv('/content/customers_dataset.csv')

df_geolocation = pd.read_csv('/content/geolocation_dataset.csv')

df_order_items = pd.read_csv('/content/order_items_dataset.csv')

df_order_payments = pd.read_csv('/content/order_payments_dataset.csv')

df_orders = pd.read_csv('/content/orders_dataset.csv')

df_reviews = pd.read_csv('/content/order_reviews_dataset.csv')

df_product_category = pd.read_csv('/content/product_category_name_translation.csv')

df_product = pd.read_csv('/content/products_dataset.csv')

df_sellers = pd.read_csv('/content/sellers_dataset.csv')

"""## Data Understanding"""

# View the first 5 data
df_customer.head()

# Display information related to the dataset

df_customer.info()

# See whether or not there are missing values

df_customer.isna().sum()

# View descriptive statistics on the dataset

df_customer.describe(exclude='number').T

# View the first 5 data
df_orders.head()

# Display information related to the dataset

df_orders.info()

# See whether or not there are missing values

df_orders.isna().sum()

"""From the table it can be seen that there are missing values in the order dataset"""

# View the first 5 data
df_order_items.head()

# Display information related to the dataset

df_order_items.info()

# See whether or not there are missing values

df_order_items.isna().sum()

# Merge customer dataset and order dataset

df_customer_order = pd.merge(df_orders, df_customer[['customer_id', 'customer_city', 'customer_state']], on="customer_id", how='left')

df_customer_order.head()

df_customer_order.describe(include="all").T

"""---

We already know that the data in the order dataset has missing values, let's find out the underlying cause by looking at the presence or absence of order items in the order dataset.
"""

# Add an isExisting feature to check for existing values

list_order_item = list(df_order_items["order_id"].unique())

df_customer_order["isExist"] = df_customer_order["order_id"].apply(lambda x: 'yes' if x in list_order_item else "no")

df_customer_order['isExist'].value_counts()

"""There are 775 orders that do not have an order item"""

df_customer_order.shape

# Merge customer order dataset and order item dataset

df_customer_order = pd.merge(df_customer_order, df_order_items, on="order_id", how='left')

df_customer_order.head()

df_customer_order.isna().sum()

"""---
Next we merge the entire dataset needed for further analysis and handling of missing values.
"""

df_customer_order = pd.merge(df_customer_order, df_order_payments, on="order_id", how='left')

df_customer_order = pd.merge(df_customer_order, df_product, on="product_id", how='left')

df_customer_order = pd.merge(df_customer_order, df_sellers, on="seller_id", how='left')

df_customer_order = pd.merge(df_customer_order, df_reviews, on="order_id", how='left')

df_customer_order.shape

df_customer_order.isna().sum()

"""## Exploratory Data Analysis (EDA)"""

# Import library

import matplotlib.pyplot as plt
import seaborn as sns

custom_palette = ['#01d1ff', "#e5c852"]

# Set the Seaborn style with a custom color palette
sns.set(style="whitegrid", palette=custom_palette)

def plot_bar(dataframe, title_, size_=(15,10), rot_=30, all='no', percentage='yes', annot_offset=1):
  """
  Plot a bar chart with annotations for 5 most high values
  """
  if all == 'no':
    dataframe = dataframe.head()

  ax = dataframe.plot(
      kind='bar',
      figsize=size_,
      rot=rot_,
      title=title_
  )

  annotate_bars(ax, dataframe, textsize=12, percentage=percentage, annot_offset=annot_offset)

  plt.ylabel(f'Count of {title_}')

  plt.show()



def annotate_bars(ax, dataframe, colour="black", textsize=12, percentage='yes', annot_offset=1):
    """
    Add value annotations to the bars
    """

    total_count = dataframe.sum()

    # Iterate over the plotted rectanges/bars
    for p in ax.patches:

        # Calculate annotation
        value = str(round(p.get_height(), 1))
        percentage_value = str(round((p.get_height() / total_count) * 100,1))

        # If value is 0 do not annotate
        if value == '0.0':
            continue
        ax.annotate(
            f'{value}\n({percentage_value}%)' if percentage == 'yes' else f'{value}',
            (p.get_x() + p.get_width()/2, p.get_y() + p.get_height() + annot_offset),
            ha='center',
            va='bottom',
            color=colour,
            size=textsize
        )

def plot_line(dataframe, title_, size_=(15,10), rot_=30, x_size=3, agg='Count'):
    """
    Plot a line chart with annotations
    """

    ax = dataframe.plot(
        kind="line",
        figsize=size_,
        rot=rot_,
        title=title_,
        marker='o',
        markerfacecolor='#01d1ff',
        markeredgecolor='#01d1ff'
    )

    annotate_points(ax, dataframe, textsize=10)

    # plot marker for max value
    max_value = dataframe.max()
    max_index = dataframe.idxmax()
    ax.plot(max_index, max_value, marker='o', markersize=8, color="red")

    ax.axvline(x=max_index, color='red', linestyle='--', linewidth=1.5)

    # label for vertical line
    ax.text(max_index, 0,
            f'{max_index.strftime("%Y-%m")}',
            color='red',
            ha='left',
            va='bottom',
            fontsize=12)

    plt.xticks(ticks=dataframe.index[::x_size], labels=dataframe.index[::x_size].strftime('%Y-%m'))

    plt.ylabel(f'{agg} of {title_}')
    plt.show()


def annotate_points(ax, dataframe, colour="black", textsize=8):
    """
    Add value annotations to the points on a line
    """

    # Find the maximum value and its corresponding index
    max_value = dataframe.max()
    max_index = dataframe.idxmax()

    # Annotate only the maximum point
    ax.annotate(
        str(round(max_value, 1)),
        (max_index, max_value),
        textcoords="offset points",
        xytext=(0, 8),  # Offset the text slightly above the point
        ha='center',
        color=colour,
        size=textsize
    )

"""### Customer and Order

***What is the distribution of customers in each state and city based on the number of orders?***
"""

# view the distribution of customer IDs based on their state
count_customer_state = df_customer_order.groupby("customer_state")["customer_id"].count().sort_values(ascending=False)

plot_bar(count_customer_state, "Customer each State")

"""From the visualization of the bar plot it can be seen that the SP state dominates the distribution of customers with an amount of about 54.5% of the total data"""

# see the distribution of customers by city with the state that dominates
count_customer_city = df_customer_order.groupby(["customer_state", "customer_city"])["customer_id"].count()

count_customer_SP_city = count_customer_city.loc["SP"].sort_values(ascending=False)

plot_bar(count_customer_SP_city, "Customer each City in State SP")

"""**conclusion**:

Based on the 2 bar plots above, it can be seen that state SP has the highest number of customers, which is around 54.5% of the total number of customers. Then for the state with the most customers, SP, the city with the most customers is sao paulo with the number of customers around 78.3% of the total number of customers in state SP.

---

1.   ***How customers grow over time based on their orders?***
2.   ***What time do customers usually place their orders?***
"""

# extracting the purchase feature

df_customer_order["purchase_time_year"] = pd.to_datetime(df_customer_order["order_purchase_timestamp"]).dt.year
df_customer_order["purchase_time_month"] = pd.to_datetime(df_customer_order["order_purchase_timestamp"]).dt.month
df_customer_order["purchase_time_day_name"] = pd.to_datetime(df_customer_order["order_purchase_timestamp"]).dt.day_name()
df_customer_order["purchase_time_hour"] = pd.to_datetime(df_customer_order["order_purchase_timestamp"]).dt.hour

# create functions for time segmentation

def time_segment(x):

  if 6 <= x < 12:
    return "morning"
  elif 12 <= x < 18:
    return "afternoon"
  elif 18 <= x <= 23:
    return "evening"
  elif 0 <= x < 6:
    return "night"

df_customer_order["purchase_time"] = df_customer_order["purchase_time_hour"].apply(lambda x: time_segment(x))

# calculate the number of customers per year

count_customer_year = df_customer_order.groupby(["purchase_time_year"])["customer_id"].count().sort_values(ascending=False)

plot_bar(count_customer_year, "Customer by Year")

"""Based on the plot bar above, it can be seen that customers have increased from year to year with the highest peak of customers in 2018 with the number of customers around 53.9% of the total customers."""

# calculate the number of customers per month from each year

df_customer_month = df_customer_order[['customer_id', 'purchase_time_year', 'purchase_time_month']].copy()

df_customer_month['year_month'] = df_customer_month['purchase_time_year'].astype('str') + '-' + df_customer_month['purchase_time_month'].astype('str')
df_customer_month['year_month'] = pd.to_datetime(df_customer_month['year_month'], format='%Y-%m')

count_customer_month = df_customer_month.groupby(["year_month"])["customer_id"].count().sort_index()

plot_line(count_customer_month, "Customer by Year-Month")

"""Based on the line plot above, it can be seen that from year to year in each month the customer has increased even though in 2018 August to September to October has decreased. The decline could be a natural occurrence or there is an error in the data (noise). then it can also be seen that the peak of the number of customers occurred in 2017 in November, although overall the largest number of customers was in 2018.

**conclusion**:

Based on the analysis above, it can be seen that customers grow every year. Although from the data it is recorded that 2018 has the most customers, but in the segmentation per month it can be seen that in November 2017 has the highest number of customers.

---
"""

# count the number of customers per day

count_customer_day = df_customer_order.groupby(['purchase_time_day_name'])['customer_id'].count().sort_values(ascending=False)

plot_bar(count_customer_day, "Customer by Day", all='yes')

"""Based on the plot bar above, it can be seen that customers usually place orders on Monday, which is around 16.3% of the total number of customers who place orders."""

# calculate the number of customers based on time segmentation per day

count_customer_hour = df_customer_order.groupby(['purchase_time'])['customer_id'].count().sort_values(ascending=False)

plot_bar(count_customer_hour, "Customer by Hour")

"""Based on the plot bar above, it can be seen that customers usually place orders during the day, which amounts to around 38.8% of the total number of customers who place orders.

**conclusion**:

Based on the analysis above, it can be seen that customers place more orders on Monday and on afternoon.

### Deep dive into customers and orders

It can be seen previously that the order dataset has missing values, especially in features related to the order track. in this section we will find out more about the underlying causes.
"""

df_customer_order.isna().sum()

count_order_status = df_customer_order.groupby(['order_status'])['customer_id'].count().sort_values(ascending=False)

plot_bar(count_order_status, "Order Status")

"""It can be seen from the plot bar that the dominating order status is delivered, which means that the ordered goods have reached the customer."""

# performs time segmentation checking based on order type

df_checking = df_customer_order[['order_status','order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'isExist']].copy()

# split the dataset into without missing values and with missing values

with_nan = df_checking[(df_checking.isna().any(axis=1))]
without_nan = df_checking[~(df_checking.isna().any(axis=1))]

with_nan['isExist'].value_counts()

"""It can be seen that the data of the order track time which has a missing value is still recorded as having an order item."""

with_nan.groupby('order_status')['isExist'].value_counts().sort_values(ascending=False)

"""It can be seen from the table above that the data is dominated by the shipped order status which still has an order item, followed by the unavailable status which does not have an order item, and so on."""

with_nan[with_nan['isExist']=='yes']['order_status'].value_counts()

"""It can also be seen from the table above that the order status that has an order item is dominated by shipped, followed by canceled, and so on.

**conclusion**:

So it can be seen that there are indeed missing values in the order data based on order statuses such as shipped, canceled, unavailable, and so on which are still reasonable.
"""

without_nan['isExist'].value_counts()

without_nan.groupby('order_status')['isExist'].value_counts().sort_values(ascending=False)

"""As expected, data with no missing values must have order items and are dominated by delivered order status."""

# counts the number of orders that have an order item

count_isExist = df_customer_order.groupby(['isExist'])['customer_id'].count().sort_values(ascending=False)

plot_bar(count_isExist, "IsExist")

"""From the visualization, it can be seen that the data that has order items dominates, which is around 99.3% of the total data. Therefore, for the next step, data that does not have missing values and the order stature is delivered will be taken. This is done for several reasons:


1. we can ignore the data that does not have an order item because it is only about 0.7% of the data.

2. if we want to fill the missing value with a value, then it doesn't make sense, because we can't get the time feature from data without order items or data with unfinished order status (shipped, canceled, unavailable, and so on).
"""

df_order_without_nan = df_customer_order[~(df_customer_order[['order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date']].isna().any(axis=1)) & (df_customer_order['order_status']=='delivered')].reset_index(drop=True)

df_order_without_nan['order_status'].value_counts()

df_order_without_nan['isExist'].value_counts()

df_order_without_nan.isna().sum()

"""---

***how long does it take for the average order to reach the customer?***
"""

# change the data type of some features

df_order_without_nan["order_purchase_timestamp"] = pd.to_datetime(df_order_without_nan["order_purchase_timestamp"])
df_order_without_nan["order_estimated_delivery_date"] = pd.to_datetime(df_order_without_nan["order_estimated_delivery_date"])
df_order_without_nan["order_delivered_customer_date"] = pd.to_datetime(df_order_without_nan["order_delivered_customer_date"])
df_order_without_nan["order_delivered_carrier_date"] = pd.to_datetime(df_order_without_nan["order_delivered_carrier_date"])
df_order_without_nan["order_approved_at"] = pd.to_datetime(df_order_without_nan["order_approved_at"])

def length_delivery(df):
  """
  Calculate the delivery duration in days
  """


  df["order_purchase_timestamp"] = df["order_purchase_timestamp"].dt.date
  df["order_estimated_delivery_date"] = df["order_estimated_delivery_date"].dt.date

  df["order_approved_at"] = df["order_approved_at"].dt.date
  df["order_delivered_carrier_date"] = df["order_delivered_carrier_date"].dt.date
  df["order_delivered_customer_date"] = df["order_delivered_customer_date"].dt.date

  try:

    delivery_time = df["order_delivered_customer_date"] - df["order_approved_at"]

    df["delivery_day_time"] = [abs(day.days) for day in delivery_time]

    return df

  except Exception as e:
    print(e)

df_order_without_nan = length_delivery(df_order_without_nan)

df_order_without_nan

df_order_without_nan['delivery_day_time'].describe().T

"""***conclusion***:

for the length of delivery to the customer, on average, it takes about 12 days in the range of a minimum of the same day or a maximum of 208 days.

***How do customers pay for their orders?***
"""

count_type_order = df_order_without_nan.groupby(['payment_type'])['customer_id'].count().sort_values(ascending=False)

plot_bar(count_type_order, "Payment Type")

"""***conclusion***:

Based on the bar plot above, it can be seen that 78.3% of customers make more payments by credit card.

***How do sales in e-commerce grow over time based on customer orders?***
"""

# melihat penjualan berdasarkan bulan dari setiap tahunnya

df_customer_spend = df_order_without_nan[['purchase_time_year', 'purchase_time_month', 'payment_value']].copy()

df_customer_spend['year_month'] = df_customer_spend['purchase_time_year'].astype('str') + '-' + df_customer_spend['purchase_time_month'].astype('str')
df_customer_spend['year_month'] = pd.to_datetime(df_customer_spend['year_month'], format='%Y-%m')

customer_spend = df_customer_spend.groupby(["year_month"])["payment_value"].sum().sort_index()

plot_line(customer_spend, "Customer Spend by Year-Month", agg='Sum')

"""**conclusion**:

Based on the above visualization, it can be seen that sales in e-commerce every month of the year tend to increase and the peak is in November 2017.

### Product
"""

df_ecommerce = df_order_without_nan

df_ecommerce.isna().sum()

"""Based on the table, it can be seen that some features in the product dataset have missing values."""

# change product name into English

def convert_english_name(x):

  if x in df_product_category['product_category_name'].values:
    return df_product_category[df_product_category['product_category_name']==x]['product_category_name_english'].values[0]

  else:
    return 'unknown'

df_ecommerce['product_category_name'] = df_ecommerce['product_category_name'].apply(lambda x: convert_english_name(x))

"""***What products sell more in e-commerce?***"""

# calculate the order quantity based on product category

count_product_order = df_ecommerce.groupby(["product_category_name"])["order_id"].count().sort_values(ascending=False)

plot_bar(count_product_order, "Product by Order")

"""We can see from the plot bar that the bed_bath_table category product has the highest number of orders, which is around 25.1%."""

# see product reviews for each product category
count_product_rating_score = df_ecommerce.groupby(["product_category_name"])["review_score"].mean().sort_values(ascending=False)

plot_bar(count_product_rating_score, "Product by Rating Score", percentage='no', annot_offset=0)

"""
Based on the bar plot, it can be seen that products in the fashion_childrens_clothes category have a better review score than other category products, although the most orders are still for products in the bed_bath_table category."""

# see the most orders based on the seller's state

count_seller_state = df_ecommerce.groupby(["seller_state"])["order_id"].count().sort_values(ascending=False)

plot_bar(count_seller_state, "Seller by State")

"""Based on the bar plot, it can be seen that sellers with SP state have the most orders, namely around 75.3%"""

# see the number of orders based on city from state SP

count_seller_city = df_ecommerce.groupby(["seller_state", "seller_city"])["order_id"].count()

count_seller_SP_city = count_seller_city.loc["SP"].sort_values(ascending=False)

plot_bar(count_seller_SP_city, "Seller each City in State SP")

"""Based on the bar plot, it can be seen that sellers from Sao Paulo City have the largest number of orders, namely around 63.7%

**conclusion**:

1. Based on the analysis above, it can be seen that customers place more orders in the bed_bath_table category products. However, products in this category have a smaller review score than products in the fashion_childrens_clothes category

2. Based on the analysis above, it can also be seen that customers buy more goods from sellers located in the SP state, specifically in the city of Sao Paulo. It can be assumed that this amount is related to the location of the customer

***How to segment customers based on behavior when placing orders in e-commerce?***
"""

# Import Library

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from category_encoders import TargetEncoder
from sklearn.metrics import silhouette_score

# take features related to customer behavior in ordering

df_segmentation = df_ecommerce[['customer_city', 'customer_state', 'payment_type', 'payment_value', 'product_category_name', 'seller_city', 'seller_state', 'purchase_time_day_name', 'purchase_time']].dropna().reset_index(drop=True)

df_segmentation_process = df_segmentation.copy()

# take features non ordinal

non_ordinal_categorical_columns = list(df_segmentation_process.select_dtypes(include='object').columns.difference(['purchase_time_day_name', 'purchase_time']))

# crate label encoder based on ordinal features

label_encoders = {}
for col in ['purchase_time_day_name', 'purchase_time']:
    le = LabelEncoder()
    df_segmentation_process[col] = le.fit_transform(df_segmentation_process[col])
    label_encoders[col] = le

# carry out standardization for numerical features
scaler = StandardScaler()
df_segmentation_process[['payment_value']] = scaler.fit_transform(df_segmentation_process[['payment_value']])

# create target encoder for non ordinal feature
target_encoder = TargetEncoder()
df_encoded = target_encoder.fit_transform(df_segmentation_process[non_ordinal_categorical_columns], df_segmentation_process['payment_value'])

df_segmentation_process.drop(columns=['customer_city', 'customer_state', 'payment_type', 'product_category_name', 'seller_city', 'seller_state'], inplace=True)

df_segmentation_process = pd.concat([df_segmentation_process, df_encoded], axis=1)

# carry out dimension reduction with PCA

pca = PCA(n_components=2)
features_pca = pca.fit_transform(df_segmentation_process)

# create inertia plot for achieve best cluster

inertia = []
k_values = range(1, 11)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(features_pca)
    inertia.append(kmeans.inertia_)


plt.figure(figsize=(10, 6))

plt.plot(k_values, inertia, 'bo-')

plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')

plt.show()

"""Based on the plot above we can take clusters with a rather slow decline, which is 3 clusters"""

k_optimal = 3
kmeans = KMeans(n_clusters=k_optimal, random_state=42)

cluster_labels = kmeans.fit_predict(features_pca)

silhouette_avg = silhouette_score(features_pca, cluster_labels)

silhouette_avg

"""1. 0.7: Strong cluster structure
2. 0.5 - 0.7: Reasonable cluster structure
3. 0.25 - 0.5: The cluster structure is weak and could be artificial
4. < 0.25: No substantial cluster structure was found

"""

df_segmentation['cluster'] = cluster_labels

df_pca = pd.DataFrame(features_pca, columns=['PCA1', 'PCA2'])
df_pca['cluster'] = df_segmentation['cluster']

plt.figure(figsize=(15, 10))

sns.scatterplot(x='PCA1', y='PCA2', hue='cluster', data=df_pca, palette='Set1', s=100, alpha=0.7)

plt.title('K-Means Clustering Visualization')

plt.xlabel('PCA1')
plt.ylabel('PCA2')
plt.legend(title='Cluster')

plt.show()

"""0.7: Strong cluster structure
0.5 - 0.7: Reasonable cluster structure
0.25 - 0.5: The cluster structure is weak and could be artificial
< 0.25: No substantial cluster structure was found
"""

df_segmentation['cluster'].value_counts()

df_segmentation

df_segmentation_cluster_0 = df_segmentation[df_segmentation['cluster']==0]
df_segmentation_cluster_1 = df_segmentation[df_segmentation['cluster']==1]
df_segmentation_cluster_2 = df_segmentation[df_segmentation['cluster']==2]

df_segmentation_cluster_0.describe(include='all').T

df_segmentation_cluster_1.describe(include='all').T

df_segmentation_cluster_2.describe(include='all').T

"""**conclusion**:


Based on the cluster above, it can be seen that:

1. For the first cluster (0), customers are mostly located in the SP state, specifically in the city of Sao Paulo and buy goods from sellers who are located in the same location. Then, in this cluster, customers mostly use credit cards to make payments. In this cluster, customers more often order products in the bed_bath_table category. For ordering times, this cluster usually orders on Thursday at noon. The average sales obtained from this cluster is around 168.7.

2. For the second cluster (1), the only difference lies in the day the customer usually places the order, namely Monday at noon. Apart from that, the average sales obtained from this cluster is around 173.4, which is the highest sales compared to other clusters.

3. For the third cluster (2), the only difference lies in the day the customer usually places the order, namely on Tuesday at noon. Apart from that, the average sales obtained from this cluster is around 172.2, which is higher than the first cluster (0)

### EXECUTIVE SUMMARY

Based on the analysis above, it can be concluded that to increase sales or transactions in e-commerce we can do the following:

**Carry out promotions on larger bed_bath_table category products to customers located in the SP state and provide discounts on Mondays. Also provide thoughtful offers to customers and sellers at that location, for example offering cheaper limited postage vouchers to attract more customer attention.**

## Data preprocessing

In this step, we will try to make predictions about the time the product ordered by the customer will arrive, because the estimates given in the dataset are deemed unreasonable, such as the goods arriving on the same day, it is estimated that they will arrive a month later.
"""

# take several features that are relevant for modeling

df_ecommerce_final = df_ecommerce[[
    'order_id',
    'customer_city',
    'customer_state',
    'payment_type',
    'payment_value',
    'product_category_name',
    'product_weight_g',
    'product_length_cm',
    'product_height_cm',
    'product_width_cm',
    'seller_city',
    'seller_state',
    'purchase_time_day_name',
    'purchase_time_hour',
    'purchase_time',
    'delivery_day_time'
]]

df_ecommerce_final.info()

df_ecommerce_final.order_id.describe().T

# drop duplication
df_ecommerce_final = df_ecommerce_final.drop_duplicates(subset=['order_id']).reset_index(drop=True)
df_ecommerce_final.shape

df_ecommerce_final.isna().sum()

# drop missing value
df_ecommerce_final = df_ecommerce_final.dropna().reset_index(drop=True)

df_ecommerce_final.isna().sum()

df_ecommerce_final = df_ecommerce_final.drop(columns=['order_id'])

df_ecommerce_final.head()

"""### Transforming Categorical Columns"""

from sklearn.preprocessing import LabelEncoder
from category_encoders import TargetEncoder

categorical_columns = df_ecommerce_final.select_dtypes(include='object')

categorical_columns.head()

non_ordinal_categorical_columns = list(categorical_columns.select_dtypes(include='object').columns.difference(['purchase_time_day_name', 'purchase_time']))

# perform transformations for non-ordinal features
target_encoder = TargetEncoder()
df_encoded = target_encoder.fit_transform(categorical_columns[non_ordinal_categorical_columns], df_ecommerce_final['delivery_day_time'])

df_encoded.head()

# perform transformations for ordinal features
label_encoders = {}
for col in ['purchase_time_day_name', 'purchase_time']:
    le = LabelEncoder()
    categorical_columns[col] = le.fit_transform(categorical_columns[col])
    label_encoders[col] = le

df_categorical = pd.concat([categorical_columns[['purchase_time_day_name', 'purchase_time']], df_encoded], axis=1).reset_index(drop=True)

df_categorical

"""### Transforming Numerical Columns"""

numerical_columns = df_ecommerce_final.columns.difference(df_categorical.columns)

df_final = pd.concat([df_ecommerce_final[numerical_columns], df_categorical], axis=1)

df_final.head()

# see the outliers in numerical features

columns_to_plot = ['delivery_day_time', 'payment_value', 'product_height_cm', 'product_length_cm', 'product_weight_g', 'product_width_cm', 'purchase_time_hour']

# Count the number of rows and columns for the subplot
n_cols = 2
n_rows = (len(columns_to_plot) + 1) // 2

fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
axes = axes.flatten()  # Flatten array of axes for ease of iteration

for i, column in enumerate(columns_to_plot):
    sns.boxplot(x=df_final[column], ax=axes[i])
    axes[i].set_title(f'Box Plot {column}')
    axes[i].set_xlabel('')

# Delete unused subplots
for i in range(len(columns_to_plot), len(axes)):
    fig.delaxes(axes[i])

plt.tight_layout()
plt.show()

def identify_outliers(df, columns):
    outlier_info = {}

    for column in columns:
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]

        outlier_info[column] = {
            'lower_bound': lower_bound,
            'upper_bound': upper_bound,
            'num_outliers': len(outliers),
            'outlier_indices': outliers.index.tolist()
        }

        print(f"\nKolom: {column}")
        print(f"Lower bound: {lower_bound}")
        print(f"Upper bound: {upper_bound}")
        print(f"Jumlah outlier: {len(outliers)}")

    return outlier_info

outlier_results = identify_outliers(df_final, columns_to_plot)

"""Outliers will be removed because there are not too many of them so as not to interfere with the modeling process"""

df_clean = df_final.copy()

all_outlier_indices = set()

for column, info in outlier_results.items():
    all_outlier_indices.update(info['outlier_indices'])

df_clean = df_clean.drop(index=list(all_outlier_indices))

print(f"\nThe amount of data before removing outliers: {len(df_final)}")
print(f"The amount of data after removing outliers: {len(df_clean)}")

df_final = df_clean.copy()

"""### Feature Selection

**categorical**

For categorical columns we will use ANOVA to perform feature selection
"""

categorical_columns = df_final[['purchase_time_day_name', 'purchase_time']].copy()

categorical_columns['purchase_time'] = categorical_columns['purchase_time'].astype('str')
categorical_columns['purchase_time_day_name'] = categorical_columns['purchase_time_day_name'].astype('str')

categorical_columns['delivery_day_time'] = df_final['delivery_day_time']

from scipy.stats import f_oneway

def one_way_anova(df, feature, target):
    groups = df.groupby(feature)[target].apply(list)
    f_value, p_value = f_oneway(*groups)
    return f_value, p_value

results = {}
for column in categorical_columns.columns.difference(['delivery_day_time']):
  f_value, p_value = one_way_anova(categorical_columns, column, 'delivery_day_time')
  results[column] = {'F-value': f_value, 'p-value': p_value}

results_df = pd.DataFrame(results).T
results_df = results_df.sort_values('p-value')

results_df

"""Features with a p-value < alpha (usually 0.05) are considered to have a significant influence on the target. Therefore, all categorical features will be used in modeling

**numerical**
"""

numerical_columns = df_final.columns.difference(['purchase_time_day_name', 'purchase_time'])

numerical_columns

correlation = df_final[numerical_columns].corr().abs()
mask = np.triu(np.ones_like(correlation, dtype=bool))

# Plot correlation
plt.figure(figsize=(45, 45))
sns.heatmap(
    correlation,
    mask=mask,
    center=0,
    xticklabels=correlation.columns.values,
    yticklabels=correlation.columns.values,
    annot=True,
    fmt='.2f',
    linewidths=1,
    annot_kws={'size': 18}
)
# Axis ticks size
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
plt.show()

corr_mask = correlation.mask(mask)

to_drop = [c for c in corr_mask.columns if any(corr_mask[c] > 0.95)]

to_drop

"""All numerical columns will be used because there is no correlation of features that exceeds 95% with the target feature, namely delivery_day_time

### Splitting Data
"""

df_final.head()

from sklearn.model_selection import train_test_split

# dividing data into, train data, validation, and testing
train_df, test_df = train_test_split(df_final, test_size=0.1, shuffle=True)
train_df, val_df = train_test_split(train_df, test_size=0.2, shuffle=True)

numerical_columns = df_final.columns.difference(['purchase_time_day_name', 'purchase_time', 'delivery_day_time'])
other_columns = df_final.columns.difference(numerical_columns)

train_df_numerical = train_df[numerical_columns].reset_index(drop=True)
train_df = train_df[other_columns].reset_index(drop=True)

val_df_numerical = val_df[numerical_columns].reset_index(drop=True)
val_df = val_df[other_columns].reset_index(drop=True)

test_df_numerical = test_df[numerical_columns].reset_index(drop=True)
test_df = test_df[other_columns].reset_index(drop=True)

# standardize numerical features so that models are more efficient in learning data

std = StandardScaler()
train_df_numerical = std.fit_transform(train_df_numerical)
val_df_numerical = std.transform(val_df_numerical)
test_df_numerical = std.transform(test_df_numerical)

train_df_numerical = pd.DataFrame(train_df_numerical, columns=numerical_columns)
val_df_numerical = pd.DataFrame(val_df_numerical, columns=numerical_columns)
test_df_numerical = pd.DataFrame(test_df_numerical, columns=numerical_columns)

train_df = pd.concat([train_df, train_df_numerical], axis=1)
val_df = pd.concat([val_df, val_df_numerical], axis=1)
test_df = pd.concat([test_df, test_df_numerical], axis=1)

"""# Modeling"""

from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import KFold, cross_val_score, RandomizedSearchCV
from sklearn.metrics import make_scorer, mean_absolute_error

X_train = train_df.drop(columns=['delivery_day_time'])
y_train = train_df['delivery_day_time']

X_val = val_df.drop(columns=['delivery_day_time'])
y_val = val_df['delivery_day_time']

X_test = test_df.drop(columns=['delivery_day_time'])
y_test = test_df['delivery_day_time']

"""In this experiment, 3 base models will be used, namely randomforest regressor, adaboost regressor, and gradientboosting regressor to choose which model is the best based on MAE scoring."""

for_ada = DecisionTreeRegressor(max_depth=1, random_state=42)

models = {
  'ada': AdaBoostRegressor(base_estimator=for_ada, random_state=42),
  'rf': RandomForestRegressor(random_state=42),
  'gb': GradientBoostingRegressor(random_state=42)
}

mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)

result = []
for model in models.values():
  kf = KFold(n_splits=6, shuffle=True, random_state=42)
  cv_results = cross_val_score(model, X_train, y_train, cv=kf, scoring=mae_scorer)
  result.append(cv_results)

plt.boxplot(result, labels=models.keys())
plt.show()

"""Based on the plot above, it can be seen that the median of MAE in the cross validation process on 6 folds in the gradient boosting model is better than other models. Therefore, hyperparameter tuning will be carried out on this model to be used later in the data training and inference processes"""

steps_gb = [('gb', GradientBoostingRegressor(random_state=42))]
pipeline_gb = Pipeline(steps=steps_gb)

parameters_gb = {
    'gb__n_estimators': [100, 200, 300, 400],
    'gb__learning_rate': [0.01, 0.1, 0.2, 0.3],
    'gb__max_depth': [3, 4, 5, 6, 7],
    'gb__min_samples_leaf': [1, 2, 3, 4]
}

kf = KFold(n_splits=6, shuffle=True, random_state=42)

cv_gb = RandomizedSearchCV(pipeline_gb, parameters_gb, cv=kf, n_iter=3, scoring='neg_mean_absolute_error')

cv_gb.fit(X_train, y_train)

print('Best score = {}'.format(cv_gb.best_score_))
print('Best parameters = {}'.format(cv_gb.best_params_))

best_model = cv_gb.best_estimator_
best_model

y_val_pred = best_model.predict(X_val)

print('mae: {}'.format(mean_absolute_error(y_val, y_val_pred)))

y_test_pred = best_model.predict(X_test)

print('mae: {}'.format(mean_absolute_error(y_test, y_test_pred)))

"""
Based on the evaluation or MAE score on validation data and testing data, it can be seen that this value is still acceptable considering the target range, namely delivery_day time is 0 to 28"""

plt.figure(figsize=(10, 6))
plt.scatter(y_val, y_val_pred, alpha=0.5)
plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)
plt.xlabel('Actual Value')
plt.ylabel('Prediction Value')
plt.title('Scatter Plot: Actual vs Prediction')
plt.tight_layout()
plt.show()

"""This plot shows that although there are some predictions that are quite close to the actual values, there are also significant prediction errors, especially at larger values. It may be worth considering further tuning the model or re-evaluating the data to improve prediction performance."""

import joblib

# save model into file
joblib.dump(cv_gb.best_estimator_, '/content/best_gb_model.pkl')